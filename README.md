# pyllama

Peque√±o cliente Python para interactuar con modelos de lenguaje mediante **ollama** (en modo local üòØ).

## Instalaci√≥n

Podemos instalar el paquete directamente desde el repositorio de GitHub con el siguiente comando:

```bash
pip install git+https://github.com/falkenslab/pyllama.git
```

Es necesario tener instalado [ollama](https://ollama.com/) ü¶ô y descargar los [modelos](https://ollama.com/search) ü§ñ que queremos utilizar:

```bash
ollama pull <modelo>
```

Por ejemplo: `ollama pull llama3.2:latest`

‚ö†Ô∏è No olvides iniciar el servicio:

```bash
ollama serve
```

## Uso

Para consultar como usar el script podemos consultar la ayuda de la siguiente forma:

```bash
$ $pyllama --help
usage: pyllama [-h] [--list] [--model MODEL] [--temperature TEMPERATURE] [--verbose] [query]

Interact√∫a con modelos de lenguaje

positional arguments:
  query                 Consulta para enviar al modelo

options:
  -h, --help            show this help message and exit
  --list                Lista de modelos disponibles
  --model MODEL         Modelo a utilizar para la consulta
  --temperature TEMPERATURE
                        Temperatura para la respuesta del modelo
  --verbose             Habilitar salida detallada
```

Para ver los modelos disponibles, podemos usar el siguiente comando:

```bash
$ pyllama --list
Modelos disponibles:
- qwen3:4b-instruct
- mistral:7b
- gemma3:latest
- llama3.2:latest
- qwen2.5-coder:latest
- deepseek-r1:8b
- gpt-oss:20b
```

Podemos hacer una consulta con el siguiente comando:

```bash
$ pyllama "¬øCu√°l es la capital de Francia?"
ü§ñ Modelo utilizado: llama3.2:latest
üó£Ô∏è Pregunta: cu√°l es la capital de Francia?

üí¨ Respuesta del modelo:
La capital de Francia es Par√≠s.


‚è±Ô∏è Tiempo transcurrido: 52.35 segundos
```

> Por defecto funciona en modo silencioso e intenta utilizar el modelo `llama3.2:latest`.

Si quieres utilizar otro modelo, puedes usar la opci√≥n `--model <modelo>`, indicando alguno de la lista anterior:

```bash
(venv) C:\Users\fvarrui\GitHub\pyllama>pyllama --model gpt-oss:20b "c√≥mo se prepara una lasa√±a"
ü§ñ Modelo utilizado: gpt-oss:20b
üó£Ô∏è Pregunta: c√≥mo se prepara una lasa√±a

üí¨ Respuesta del modelo:
### Lasa√±a cl√°sica (de carne)

#### Ingredientes (para 6-8 personas)

| Ingrediente | Cantidad |
|-------------|----------|
| L√°minas de lasa√±a (pre-cocidas o normales) | 12-15 |
| Carne molida (res o mezcla res/pollo) | 500‚ÄØg |
| Cebolla | 1 grande, picada |
| Ajo | 2 dientes, picados |
| Zanahoria | 1, rallada (opcional) |
| Apio | 1 tallo, picado (opcional) |
| Tomate triturado | 400‚ÄØg (o salsa de tomate) |
| Aceite de oliva | 2‚ÄØcucharadas |
| Sal, pimienta | al gusto |
| Hierbas italianas (or√©gano, albahaca) | 1‚ÄØcucharadita |
| Queso mozzarella | 200‚ÄØg, rallado |
| Queso parmesano | 50‚ÄØg, rallado |
| Bechamel (salsa blanca) | 400‚ÄØml |
| Mantequilla | 30‚ÄØg |
| Harina | 30‚ÄØg |
| Leche | 400‚ÄØml |
| Nuez moscada | una pizca |
| Aceitunas negras (opcional) | 10, troceadas |
| Hojas de albahaca fresca (para decorar) | al gusto |

#### Instrucciones

1. **Preparar la salsa bolo√±esa**
   - En una sart√©n grande, calienta el aceite de oliva a fuego medio.
   - A√±ade la cebolla y el ajo; sofr√≠e 3‚Äì4‚ÄØmin hasta que est√©n transparentes.
   - Incorpora la carne molida y cocina hasta que pierda el color rosado.
   - Si usas zanahoria y apio, agr√©galos ahora y cocina 2‚ÄØmin m√°s.
   - Vierte el tomate triturado, a√±ade sal, pimienta, or√©gano y albahaca.
   - Deja cocer a fuego lento 15‚Äì20‚ÄØmin, removiendo de vez en cuando.
   - Ajusta de sal y pimienta.

2. **Preparar la bechamel**
   - En una cacerola, derrite la mantequilla a fuego medio.
   - A√±ade la harina y cocina 1‚ÄØmin sin dejar que se dore.
   - Vierte la leche poco a poco, batiendo constantemente para evitar grumos.
   - Cocina hasta que la salsa espese (aprox. 5‚ÄØmin).
   - A√±ade una pizca de nuez moscada, sal y pimienta.
   - Reserva.

3. **Montar la lasa√±a**
   - Precalienta el horno a 180‚ÄØ¬∞C (350‚ÄØ¬∞F).
   - En una fuente rectangular (aprox. 30‚ÄØ√ó‚ÄØ20‚ÄØcm), unta una capa fina de bechamel.
   - Coloca una capa de l√°minas de lasa√±a (pueden ser crudas si tu paquete indica que se cocinan en el horno).
   - A√±ade una capa de salsa bolo√±esa, una capa de bechamel y espolvorea queso mozzarella.
   - Repite las capas hasta terminar con una capa de bechamel y una generosa cantidad de mozzarella y parmesano.
   - Si deseas, a√±ade aceitunas negras en la √∫ltima capa para un toque extra de sabor.

4. **Hornear**
   - Cubre la fuente con papel de aluminio (para evitar que el queso se queme).
   - Hornea 30‚ÄØmin.
   - Retira el papel y hornea 10‚Äì15‚ÄØmin m√°s, o hasta que la superficie est√© dorada y burbujeante.

5. **Reposar y servir**
   - Deja reposar la lasa√±a 10‚ÄØmin antes de cortar.
   - Decora con hojas de albahaca fresca.
   - Sirve acompa√±ada de una ensalada verde o pan de ajo.

#### Consejos

- **L√°minas pre-cocidas**: Si usas l√°minas que ya est√°n precocidas, solo necesitas una capa de salsa bolo√±esa y bechamel antes de hornear.
- **Versi√≥n vegetariana**: Sustituye la carne por champi√±ones salteados, berenjena, calabac√≠n y espinacas.
- **Salsa de tomate casera**: Si prefieres usar tomate fresco, hierve y lic√∫a los tomates, luego coc√≠nalos con cebolla, ajo y hierbas.
- **Sabor extra**: A√±ade un chorrito de vino tinto a la bolo√±esa mientras se cocina.

¬°Disfruta de tu lasa√±a casera!


‚è±Ô∏è Tiempo transcurrido: 1001.82 segundos
```

Y si quieres que d√© m√°s informaci√≥n de lo que est√° haciendo (uso de funciones - tools), utiliza el modificador `--verbose`:

```bash
$ $pyllama --verbose "si tengo 3 establos, y en cada establo tengo 5 burros, y compro 2 burros m√°s, cu√°ntos burros tengo en total?"
ü§ñ Modelo utilizado: llama3.2:latest
üó£Ô∏è Pregunta: si tengo 3 establos, y en cada establo tengo 5 burros, y compro 2 burros m√°s, cu√°ntos burros tengo en total?

üõ†Ô∏è Herramientas disponibles:
        * get_current_time: Obtiene la fecha y hora actual del sistema.
        * get_weather: Obtiene informaci√≥n meteorol√≥gica para una ciudad espec√≠fica.
        * calculate_math: Calcula una expresi√≥n matem√°tica simple de forma segura. Esta funci√≥n eval√∫a...
        * list_files: Lista los archivos en un directorio espec√≠fico. Esta funci√≥n obtiene una...

‚öôÔ∏è Llamando funci√≥n: calculate_math({'expression': '(3*5)+2'})
‚úÖ Resultado: {'expression': '(3*5)+2', 'result': 17}

üí¨ Respuesta del modelo:
Si tienes 3 establos y en cada establo hay 5 burros, eso significa que tienes un total de 3 x 5 = 15 burros.

Luego, compras 2 burros m√°s, por lo que ahora tienes 15 + 2 = 17 burros.


‚è±Ô∏è Tiempo transcurrido: 57.50 segundos
```

## Funciones disponibles para los modelos

Los modelos tienen acceso a las siguientes funciones (tools) que pueden utilizar autom√°ticamente seg√∫n sea necesario:

| Funci√≥n            | Descripci√≥n                                                                                                       |
| ------------------ | ----------------------------------------------------------------------------------------------------------------- |
| `get_current_time` | Obtiene la fecha y hora actual del sistema en formato "YYYY-MM-DD HH:MM:SS"                                       |
| `get_weather`      | Obtiene informaci√≥n meteorol√≥gica simulada para una ciudad espec√≠fica (temperatura, condici√≥n clim√°tica, humedad) |
| `calculate_math`   | Calcula expresiones matem√°ticas simples de forma segura (operaciones b√°sicas: +, -, *, /, par√©ntesis)             |
| `list_files`       | Lista los archivos y directorios en una ruta espec√≠fica (m√°ximo 10 elementos)                                     |

√âstas se encuentran definidas en `src/pyllama/functions.py`.

### Ejemplos de uso con funciones

```bash
# El modelo usar√° autom√°ticamente get_current_time
$ pyllama "¬øQu√© hora es?"

# El modelo usar√° get_weather
$ pyllama "¬øC√≥mo est√° el clima en Madrid?"

# El modelo usar√° calculate_math
$ pyllama "Calcula 25 * 30 + 100"

# El modelo usar√° list_files
$ pyllama "¬øQu√© archivos hay en el directorio actual?"
```

## Para desarrolladores

Clonamos el repositorio y nos movemos al directorio del proyecto:

```bash
git clone https://github.com/falkenslab/pyllama.git
cd pyllama
```

Crearemos un entorno virtual:

```bash
python -m venv venv
```

Lo activamos:

```bash
venv\Scripts\activate  # En Windows
# o
source venv/bin/activate  # En Linux o macOS
```

Instalamos las dependencias necesarias en modo de edici√≥n:

```bash
pip install -e .
```

Y a programar:

```bash
code .
```

## Mejoras

Podemos incorporar m√°s funciones para que el modelo pueda realizar m√°s tareas o acciones, como hacer b√∫squedas web, consultar alguna API, conectarse a una base de datos, realizar tareas en el sistema (crear/eliminar archivos, instalar/desinstalar apps, comprobar que procesos se est√°n comiendo tu RAM y tu CPU üòÅ...).

